---
title: "UK - Decision Tree"
author: "Author_Name"
date: "5/20/2021"
output: html_document
---

### Load packages
```{r message=FALSE}
my_pack <- function(package)
  {
  new_packages <- package[!(package %in% installed.packages()[, "Package"])]
  if (length(new_packages)) 
    install.packages(new_packages, dependencies = T)
  sapply(package, require, character.only = T)
}

packages <- c("tidyr", "dplyr", "knitr","randomForest","tibble",  "plotROC", "corrplot",
              "fastDummies", "plotly", "FactoMineR","ggplot2", "factoextra",
              "rpart", "rpart.plot",  "caret", "kernlab", "gridExtra")
my_pack(packages)
```


###Load the 'cleaned' file
                                                                       
```{r}
data_accidents <- read.csv (file.choose(), header = TRUE)
data_accidents <- as_tibble(data_accidents[ , !(colnames(data_accidents) %in% c('Accident_Index', 'Latitude',
                                                                             'Datetime','Longitude'))])
data_accidents <- data_accidents %>% rename(`Severity_accident`=`Accident_Severity`,
                                            `Rural_Urban` = `Urban_or_Rural_Area`)
```

###Sampling the data to just 20000 observations                                                                       
```{r}
set.seed(1235)
sampled_categorical <- sample_n(data_accidents, 20000)
```

###splitting the data into 70-30, where training-70 and test-30.                                                                       
```{r}
set.seed(2355)
data_sample <- createDataPartition(sampled_categorical$Severity_accident, list=F, p=0.7) 
train_data <- sampled_categorical[data_sample, ]
test_data <- sampled_categorical[-data_sample, ]
```

#### Baseline Accuracy                                                                       
```{r}
base_accuracy <- round(length(test_data[test_data$Severity_accident=='Slight',]$Severity_accident)/length(test_data$Severity_accident), 3)
base_accuracy
```
## Training
### Decision Tree
#### Training the unpruned and post-pruned models
```{r}

options(warn=-1)

start <- Sys.time()

Upruned_decisiontree <- rpart(Severity_accident ~ ., data = train_data,
                               minbucket=1, method="class", control=rpart.control(cp=0.0001), minsplit=1)
```


#### Unpruned Decision tree                                                                     
```{r}
png("Unpruned DecisionTree1.png", width=1910, height=1075, res=450)

rpart.plot(Upruned_decisiontree, main = "Unpruned - Road accident severity in UK",
           branch.lty=3, extra=104, type=3, split.cex=1.2)
```


#### Unpruned accuracy                                                                   
```{r}
dev.off()
test_data$Predict <- predict(Upruned_decisiontree, test_data, type="class")
end <- Sys.time()
Runtime <- round(as.numeric(end - start)/60, 2)

####Unpruned accuracy 
unpruned_accuracy <- round(mean(test_data$Predict==test_data$Severity_accident),3)
unpruned_accuracy
```

####  DecisionTree before Pruning                                                               
```{r}
c_p <- as_tibble(Upruned_decisiontree$cptable) %>%
  filter(xerror == max(xerror)) %>%
  filter(xerror <= min(xerror) + xstd) %>%
  select(CP) %>%
  unlist()
prun_decisiontree <- prune(Upruned_decisiontree, cp=c_p)
rpart.plot(prun_decisiontree, main="Pruned - Road accident severity in UK",
           split.cex=1.2, extra=104,branch.lty=3,  type=3)
```

##### Pruned_Tree Accuracy                                                                       
```{r}

test_data$Predict <- predict(prune_decisiontree, test_data, type="class")
pruned_accuracy <- round(mean(test_data$Predict == test_data$Severity_accident), 3)

#compare accuracy
data.frame(base_accuracy, unpruned_accuracy, pruned_accuracy)


str(test_data)
test_data$Severity_accident = as.factor(test_data$Severity_accident)

df_confusion_mtrx <- confusionMatrix(data=test_data$Predict, reference=test_data$Severity_accident)
test_data <- test_data %>%
  mutate(Predict = predict(prune_decisiontree, type="class", test_data),
         Predict_prob = predict(prun_decisiontree, type="prob", test_data)[,2],
         error = ifelse(Predict != Severity_accident, 1, 0))
```

                                                                       
```{r}
roc_d <- test_data %>%
  select(Severity_accident, Predict_prob) %>%
  mutate(Severity_accident = as.numeric(Severity_accident) - 1,
         Severity_accident.str = c("Fatal_Serious", "Slight")[Severity_accident + 1]) %>%
  ggplot(aes(d = Severity_accident, m = Predict_prob)) +
  geom_roc(labels = F)
roc_d +
  style_roc(xlab = "False Positive Rate", theme = theme_bw, ylab = "True Positive Rate") +
  theme(panel.grid.major = element_blank(), panel.border = element_blank(),
        axis.line = element_line(colour = "grey")) +
  ggtitle("Decision Tree - ROC Curve") +
  annotate("text", y = .25, x = .75,
           label = paste("AUROC =", round(calc_auc(roc_d)$AUC, 3)))
```


```{r}
Recall_df <- round(df_conf_mtrx$byClass['Sensitivity'], 3)

df_accuracy <- round(df_conf_mtrx$overall['Accuracy'], 3)

Presn_df <- round(df_conf_mtrx$byClass['Pos Predict Value'], 3)

ROC_df <- round(calc_auc(roc_d)$AUC, 3)

F1score_df <- round(2*((Presn_df * Recall_df) / (Presn_df + Recall_df)), 3)

test_data <- test_data[ , !(names(test_data) %in% c('Predict', 'Predict_prob', 'error'))]

data.frame(base_accuracy, df_accuracy, Presn_df, Recall_df, F1score_df, ROC_df)
```

### Importance Variable in Decision Tree 
```{r}

df_imp <- data.frame(Var=names(prun_decisiontree$variable.importance),
                            Var_Imp=prun_decisiontree$variable.importance)
df_imp_plot <- ggplot(data = df_imp, aes(x=reorder(Var, fill=Var_Imp,
                                                   Var_Imp), y=Var_Imp)) +
  geom_bar(stat = 'identity') +  labs(x="") +coord_flip() + + theme(legend.position="none")
  + theme(plot.title=element_text(hjust=0.6)) + ggtitle('Decision Tree model Variable Importance')
df_imp_plot

rm(df_imp, c_p )
```
